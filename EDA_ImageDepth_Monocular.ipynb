{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71ac2a17-837f-45bd-96d6-06e2628306d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dilip/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "c:\\Users\\dilip\\anaconda3\\envs\\radarviz\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\dilip\\anaconda3\\envs\\radarviz\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Using cache found in C:\\Users\\dilip/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load MiDaS model\n",
    "model_type = \"DPT_Large\"  # or \"MiDaS_small\"\n",
    "model = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "model.eval()\n",
    "\n",
    "# Load transforms\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "transform = midas_transforms.dpt_transform if model_type.startswith(\"DPT\") else midas_transforms.small_transform\n",
    "\n",
    "# Load image with OpenCV and convert to RGB\n",
    "img_cv = cv2.imread(\"C:/Users/dilip/Projects/Jupyter/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151609012404.jpg\")\n",
    "img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Apply MiDaS transform (expects NumPy array)\n",
    "input_tensor = transform(img_rgb)\n",
    "\n",
    "# Predict depth\n",
    "with torch.no_grad():\n",
    "    prediction = model(input_tensor)\n",
    "    depth = prediction.squeeze().cpu().numpy()\n",
    "\n",
    "# Resize depth to match original image\n",
    "depth_resized = cv2.resize(depth, (img_cv.shape[1], img_cv.shape[0]))\n",
    "\n",
    "# Normalize and visualize\n",
    "depth_vis = cv2.normalize(depth_resized, None, 0, 255, cv2.NORM_MINMAX)\n",
    "#cv2.imshow(\"Depth Map\", depth_vis.astype(np.uint8))\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9f0e17-aefb-434f-9c51-aa2d159c2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image with OpenCV and convert to RGB\n",
    "img_cv = cv2.imread(\"C:/Users/dilip/Projects/Jupyter/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151609012404.jpg\")\n",
    "img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Apply MiDaS transform (expects NumPy array)\n",
    "input_tensor = transform(img_rgb)\n",
    "\n",
    "# Predict depth\n",
    "with torch.no_grad():\n",
    "    prediction = model(input_tensor)\n",
    "    depth = prediction.squeeze().cpu().numpy()\n",
    "\n",
    "# Resize depth to match original image\n",
    "depth_resized = cv2.resize(depth, (img_cv.shape[1], img_cv.shape[0]))\n",
    "\n",
    "# Normalize and visualize\n",
    "depth_vis = cv2.normalize(depth_resized, None, 0, 255, cv2.NORM_MINMAX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5321747d-e502-4068-8a56-7c5dbb9424e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "depth_vis = depth_vis.astype(np.uint8)\n",
    "depth_vis_color = cv2.cvtColor(depth_vis, cv2.COLOR_GRAY2BGR)\n",
    "cv2.imshow(\"Depth Map with Points\", depth_vis_color)\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey(0)\n",
    "    if key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    else:\n",
    "        print(\"Press 'q' to exit.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf44a4a3-7bd6-486f-9df0-bc61b8dcda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize depth image if needed to match original\n",
    "if depth_vis_color.shape != img_cv.shape:\n",
    "    depth_vis_color = cv2.resize(depth_vis_color, (img_cv.shape[1], img_cv.shape[0]))\n",
    "\n",
    "# Stack side by side\n",
    "combined = np.hstack((img_cv, depth_vis_color))\n",
    "\n",
    "# Show the result\n",
    "cv2.imshow(\"Original and Depth Side by Side\", combined)\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey(0)\n",
    "    if key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    else:\n",
    "        print(\"Press 'q' to exit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb6448-c1a9-442b-bc1c-11a201ab8bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (radarviz)",
   "language": "python",
   "name": "radarviz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
